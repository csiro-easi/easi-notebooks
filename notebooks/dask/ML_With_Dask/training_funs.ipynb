{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7b8f-412f-4359-bc79-6c272b7cfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on A's work\n",
    "def train(zarr_path, s3_bucket, output_dir, log_dir, checkpoint_file=None):\n",
    "\n",
    "    start_epoch= 0\n",
    "    num_epochs = 10\n",
    "    batch_size = 8\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    worker_rank = int(dist.get_rank())\n",
    "    device = torch.device(0)\n",
    "    \n",
    "    #Dataset\n",
    "    train_ds = WorldCoverZarrDataset(zarr_path, patch_size=224, num_classes=11)\n",
    "    val_ds   = WorldCoverZarrDataset(zarr_path, patch_size=224, num_classes=11)\n",
    "\n",
    "    sampler_train = DistributedSampler(train_ds)\n",
    "    sampler_val   = DistributedSampler(val_ds)\n",
    "\n",
    "    traingen = DataLoader(train_ds, batch_size=batch_size, sampler=sampler_train, shuffle=False)\n",
    "    valgen   = DataLoader(val_ds,   batch_size=batch_size, sampler=sampler_val, shuffle=False)\n",
    "    \n",
    "    val_loss_min = np.Inf\n",
    "    \n",
    "    # pass to GPU device -> wrapped in DDP then it can communicate with the other workers\n",
    "    model = PrithviSegmentation(num_classes=11)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model = DDP(model, device_ids=[0])\n",
    "    model_without_ddp = model.module\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "    if checkpoint_file:\n",
    "        checkpoint_connection = S3Checkpoint(region='ap-southeast-2')\n",
    "        # load best saved model checkpoint from previous commit (if present)\n",
    "        with checkpoint_connection.reader(f\"s3://{bucket}/{userid}/{project_name}/\"+checkpoint_file) as reader:\n",
    "            checkpoint = torch.load(reader)\n",
    "        model_without_ddp.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        val_loss_min =checkpoint['val_loss_min']\n",
    "        \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        log={}\n",
    "        log.update({\"worker_rank\":worker_rank, \"epoch\":epoch, \"start_time\":datetime.now().isoformat()})\n",
    "        \n",
    "        train_accuracy = 0\n",
    "        train_loss = 0\n",
    "        val_accuracy = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        sampler_train.set_epoch(epoch)\n",
    "        sampler_val.set_epoch(epoch)\n",
    "        # #Training\n",
    "        model.train()\n",
    "        for inputs, labels in traingen:\n",
    "            inputs = inputs.to(device)# (B, 6, 3, 224, 224)\n",
    "            labels = labels.to(device)# (B, 224, 224)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)# (B, 11, 224, 224)\n",
    "            # pixel accuracy (ignoring 255)\n",
    "            with torch.no_grad():\n",
    "                preds = outputs.argmax(dim=1)# (B, H, W)\n",
    "                valid = labels != 255\n",
    "                correct = (preds[valid] == labels[valid]).sum().item()\n",
    "                total = valid.sum().item()\n",
    "                acc = correct / max(total, 1)\n",
    "    \n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "            train_accuracy += acc\n",
    "        \n",
    "        log.update({\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"train_accuracy\": float(train_accuracy),\n",
    "            \"end_time_train\": datetime.now().isoformat(),\n",
    "        })\n",
    "    \n",
    "        # --- VAL ---\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valgen:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "    \n",
    "                outputs = model(inputs)\n",
    "    \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                valid = labels != 255\n",
    "                correct = (preds[valid] == labels[valid]).sum().item()\n",
    "                total = valid.sum().item()\n",
    "                acc = correct / max(total, 1)\n",
    "    \n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                val_accuracy += acc\n",
    "        \n",
    "        log.update({\n",
    "            \"val_loss\": float(val_loss),\n",
    "            \"val_accuracy\": float(val_accuracy),\n",
    "            \"end_time_val\": datetime.now().isoformat(),\n",
    "        })\n",
    "    \n",
    "        dist.barrier()\n",
    "        if worker_rank == 0:\n",
    "            log_file = log_dir + f\"log_epoch_{epoch}.json\"\n",
    "            boto3.client(\"s3\").put_object(\n",
    "                Body=json.dumps(log),\n",
    "                Bucket=s3_bucket,\n",
    "                Key=log_file,\n",
    "            )\n",
    "            if val_loss < val_loss_min:\n",
    "                val_loss_min = val_loss\n",
    "                output_file=output_dir+ 'model_{}.pth'.format(epoch)\n",
    "                checkpoint_connection = S3Checkpoint(region='ap-southeast-2')\n",
    "                checkpoint = {'model': model_without_ddp.state_dict(), 'optimizer': optimizer.state_dict(),'epoch': epoch, 'val_loss_min':val_loss_min}\n",
    "                with checkpoint_connection.writer(f\"s3://{bucket}/{userid}/{project_name}/\"+output_file) as writer:\n",
    "                    torch.save(checkpoint, writer)\n",
    "\n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
