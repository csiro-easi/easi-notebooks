{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e374c84f-75e4-4859-bf4f-3a7847aef454",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Go Big or Go Home Part 2 - Working and Visualising on cluster <img align=\"right\" src=\"../../resources/csiro_easi_logo.png\">\n",
    "\n",
    "In this notebook we finally do our larger area. We're going to need some better visualisation tools and it would be great not to bring the results back to the Jupyter notebook but to leverage the dask clusters resources during visualisation. We'll be using some dask-aware visualiation libraries (holoviews and datashader) to do the heavy lifting.\n",
    "\n",
    "Let's begin by starting up our cluster and sizing it appropriately to our computational task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770317f-ec1b-4365-9a23-d4ef684a1a11",
   "metadata": {},
   "source": [
    "## Time to go big!\n",
    "\n",
    "All the code here is the same as the conclusion from the previous notebook, except we'll make the cluster bigger with 10 workers instead of 4. We'll also make the masking and NDVI calculation into a python function since we won't be making any changes to that now.\n",
    "\n",
    "We'll use the same ROI and time period for this run and we're using all the techniques so far to reduce the computation time:\n",
    "1. Dask chunk size selection\n",
    "2. Only loading the measurements we intend on using in this calculation to save on the task graph optimisation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619a66a-c80f-4345-8022-a81018dc294f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Gateway client\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "number_of_workers = 10 \n",
    "\n",
    "gateway = Gateway()\n",
    "\n",
    "clusters = gateway.list_clusters()\n",
    "if not clusters:\n",
    "    print('Creating new cluster. Please wait for this to finish.')\n",
    "    cluster = gateway.new_cluster()\n",
    "else:\n",
    "    print(f'An existing cluster was found. Connecting to: {clusters[0].name}')\n",
    "    cluster=gateway.connect(clusters[0].name)\n",
    "\n",
    "cluster.scale(number_of_workers)\n",
    "\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c6ebaa-8f17-4767-b120-8917bd75a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "pyproj.set_use_global_context(True)\n",
    "\n",
    "import git\n",
    "import sys, os\n",
    "from dateutil.parser import parse\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import datacube\n",
    "from datacube.utils import masking\n",
    "from datacube.utils.aws import configure_s3_access\n",
    "\n",
    "# EASI defaults\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "repo = git.Repo('.', search_parent_directories=True).working_tree_dir\n",
    "if repo not in sys.path: sys.path.append(repo)\n",
    "from easi_tools import EasiDefaults, notebook_utils\n",
    "easi = EasiDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eaf35c-f851-4121-b3dc-fe34ee025e0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dc = datacube.Datacube()\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9b521-6b25-4738-be07-b1d575a7f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the centroid of the coordinates of the default extents\n",
    "central_lat = sum(easi.latitude)/2\n",
    "central_lon = sum(easi.longitude)/2\n",
    "# central_lat = -42.019\n",
    "# central_lon = 146.615\n",
    "\n",
    "# Set the buffer to load around the central coordinates\n",
    "# This is a radial distance for the bbox to actual area so bbox 2x buffer in both dimensions\n",
    "buffer = 0.8\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "# Data product\n",
    "products = easi.product('landsat')\n",
    "\n",
    "# Set the date range to load data over\n",
    "set_time = easi.time\n",
    "set_time = (set_time[0], parse(set_time[0]) + relativedelta(years=1))\n",
    "# set_time = (\"2021-01-01\", \"2021-12-31\")\n",
    "\n",
    "# Selected measurement names (used in this notebook)\n",
    "alias = easi.aliases('landsat')\n",
    "measurements = [alias[x] for x in ['qa_band', 'red', 'nir']]\n",
    "\n",
    "# Set the QA band name and mask values\n",
    "qa_band = alias['qa_band']\n",
    "qa_mask = easi.qa_mask('landsat')\n",
    "\n",
    "# Set the resampling method for the bands\n",
    "resampling = {qa_band: \"nearest\", \"*\": \"average\"}\n",
    "\n",
    "# Set the coordinate reference system and output resolution\n",
    "set_crs = easi.crs('landsat')  # If defined, else None\n",
    "set_resolution = easi.resolution('landsat')  # If defined, else None\n",
    "# set_crs = \"epsg:3577\"\n",
    "# set_resolution = (-30, 30)\n",
    "\n",
    "# Set the scene group_by method\n",
    "group_by = \"solar_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2febae5-307c-4f16-ada5-1236d4a46ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_seasonal_ndvi(dataset):\n",
    "    # Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "    cloud_free_mask = masking.make_mask(dataset[qa_band], **qa_mask)\n",
    "    # Apply the mask\n",
    "    cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "    # Calculate the components that make up the NDVI calculation\n",
    "    band_diff = cloud_free[alias['nir']] - cloud_free[alias['red']]\n",
    "    band_sum = cloud_free[alias['nir']] + cloud_free[alias['red']]\n",
    "    # Calculate NDVI\n",
    "    ndvi = None\n",
    "    ndvi = band_diff / band_sum\n",
    "\n",
    "    return ndvi.groupby(\"time.season\").mean(\"time\")  # Calculate the seasonal mean\n",
    "\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling=resampling,\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = {\"time\":2, \"x\":3072, \"y\":3072},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "ndvi_unweighted = masked_seasonal_ndvi(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6df3e-d808-47e9-a9a0-cf7328253c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"dataset size (GiB) {dataset.nbytes / 2**30:.2f}\")\n",
    "print(f\"ndvi_unweighted size (GiB) {ndvi_unweighted.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c099d-de4d-42c8-9eef-f9f01b34bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.wait_for_workers(n_workers=10)  # Before release 2023.10.0\n",
    "client.sync(client._wait_for_workers,n_workers=10) # Since release 2023.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee9be6-9c84-4eff-91a0-d876cbfd99e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea43a7-ec03-4413-89b1-91e5fbeabee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a501db5-d805-4b26-b511-ec2f8d169a52",
   "metadata": {
    "tags": []
   },
   "source": [
    "You'll notice the computation time is slightly faster with more workers - we're IO bound so more workers means more available IO bandwidth and threads. It's not 2-3 x faster though - we're wasting a lot of resources because we can't actually use all of that extra power.\n",
    "\n",
    "> __Tip__: More isn't always better. Be mindful of your computational resource usage and cost. This size cluster is a tremendous waste for this size computational job. Size things appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5ee837-faae-4928-b9a4-dbe6d0d43476",
   "metadata": {},
   "source": [
    "And visualise the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c304d8-407d-48e3-95cd-41aa51537382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_result.sel(season='DJF').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495ba8f-301c-4751-af73-a5333348e2bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "We'll save the coordinates of this section from the array (as slices) so we can use them later for visualising the same ROI from a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dc7f0e-b2f0-4093-9d81-7cdc00cd8afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_slice = slice(ndvi_unweighted.x[0], ndvi_unweighted.x[-1])\n",
    "y_slice = slice(ndvi_unweighted.y[0], ndvi_unweighted.y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbfb392-9ffd-418e-a610-e29503980458",
   "metadata": {},
   "source": [
    "## Now for a bigger area\n",
    "\n",
    "Let's change the area extent to about 4 degrees square.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08de486-3ff3-44c3-8d0e-29dfbd929982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the bounding box for the study area\n",
    "buffer = 2\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfb47e-53ec-4e77-8f20-bc687c8f35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the map below to see if you are including the area that you want. For this example, it would be best to not include too much water.\n",
    "from dea_tools.plotting import display_map\n",
    "display_map(study_area_lon, study_area_lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a90609-1d18-4ab9-ab8c-ce83659db9e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling=resampling,\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = {\"time\":2, \"x\":3072, \"y\":3072},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "ndvi_unweighted = masked_seasonal_ndvi(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449e859a-5cbc-4341-ae78-d5735b11ed35",
   "metadata": {},
   "source": [
    "Before we compute anything let's take a look at our result's shape and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66426b-ef44-469a-9675-116f5d3fec1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"dataset size (GiB) {dataset.nbytes / 2**30:.2f}\")\n",
    "print(f\"ndvi_unweighted size (GiB) {ndvi_unweighted.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07021f2b-5f82-4a61-9afb-c8e5b304b94e",
   "metadata": {},
   "source": [
    "This is now much bigger!\n",
    "\n",
    "The result is getting on the large size for the notebook node __so we will need to pay attention to _data locality_ and the size of results being processed__. The cluster has a LOT more memory than the _notebook node_; bring too much back to the notebook and the notebook will crash.\n",
    "\n",
    "> __Tip__: Be mindful of the size of the results and their _data locality_. \n",
    "\n",
    "Now let's check the _shape_, _tasks_ and _chunks_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751b9c3-75b6-4143-a484-68d9fab6751f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca340a5-1068-4cdc-b4ae-ed7eda75853e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Looking at the `red` data variable we can see about 50 GiB for the array, 36 MiB per chunk and 5526 tasks. Noting the `nir` and `qa_band` will be similarly shaped and size.\n",
    "\n",
    "The number of tasks is climbing so we can expect an increase in _task graph optimisation_ time.\n",
    "\n",
    "Chunk size and tasks seems okay, but we will monitor the _dask dashboard_ in case there are issues with temporaries causing _workers_ to _spill to disk_ if memory is too full.\n",
    "\n",
    "_The chunking is resulting in some slivers, particularly on the y axis._ Let's modify the y chunk size so these slivers don't exist as its blowing out the tasks and is likely unnecessary. We will calculate the x and y chunk sizes below to get a nice fit. _Make sure to check the chunk size afterwards to make sure it doesn't get too large. If it does we can make the chunks smaller to reduce slivers too._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff1968-87fb-4b02-8957-b78348a30d52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "y_chunks = ceil(dataset.dims['y']/5)\n",
    "y_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2785d84-c578-40be-898f-84536ccc45a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling=resampling,\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = {\"time\":2, \"x\":3072, \"y\":y_chunks},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "ndvi_unweighted = masked_seasonal_ndvi(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f8379-60e4-4fd7-b722-bfeb1b09fc18",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now recheck our chunk size and tasks for `red`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd593d-741b-423f-95cd-3c501230a6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ea5908-12e0-4c89-af9e-b09a905e1638",
   "metadata": {
    "tags": []
   },
   "source": [
    "Very marginal increase in _memory per chunk_ but the _tasks_ have dropped from 5526 to 4661. Note that this occurs for every measurement and operation so the benefit is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12283353-1f88-4abd-b8de-fc490ea02cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7438cf-1d36-4afc-9230-744765bc11cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Total task count is sub 100_000 so should be okay but _task graph optimisation_ will take a while. Resulting array is a bit big for the notebook node as stated previously.\n",
    "\n",
    "The shape spatially is `y:15686, x:13707`. Standard plots aren't going to work very well for visualising the result in the notebook and the result uses a fair amount of memory so we'll need a different approach.\n",
    "\n",
    "For now let's visualize the same ROI as the small area before. We stashed that ROI in `x_slice, y_slice`.\n",
    "\n",
    "__If you haven't already, open the dask dashboard so you can watch the cluster make progress__\n",
    "\n",
    "The code to do this visualisation is basically the same as before except now we specify a slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9258c-010b-4a5c-92a7-1e88c8f09fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ndvi_unweighted.sel(season='DJF', x=x_slice, y=y_slice).compute().plot(robust=True, size=6, aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e9086-b7b9-490c-88a8-a36e4c272862",
   "metadata": {},
   "source": [
    "The computation time is relatively short since we are only materialising the result for a subset of the overall dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecd4988-8843-4cc2-8d82-7be1b63f6b92",
   "metadata": {},
   "source": [
    "## Visualising all of the data\n",
    "\n",
    "To visualise all of the data we will make use of the dask cluster and some dask-aware visualisation capabilties from `holoviews` and `datashader` python libraries. These libraries provide an _interactive_ visualisation capability that leaves the large datasets on the cluster and transmits only the final visualisation to the Jupyter notebook. This is done on the fly so the user can zoom and pan about the dataset in all dimensions and the dask cluster will scale data to fit in the viewport automatically. Details of how this is done and advanced features available is beyond the scope of this dask and ODC course but the manuals are extensive and the basic example here both powerful and useful.\n",
    "\n",
    "> __Tip__: The [datashader pipeline](https://datashader.org/getting_started/Pipeline.html) page provides an excellent summary of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9c328-5e93-4aed-bb4f-0f6aab70c71d",
   "metadata": {},
   "source": [
    "### `compute()` and `persist()`\n",
    "\n",
    "The first thing we will do is `persist()` the results of our calculation to the cluster. This will materialise the results but will keep the result on the cluster (so all lazy tasks are calculated, just like `compute()` but _data locality_ remains on the cluster). This will ensure the result is readily available for the visualisation. The cluster has plenty of (distributed) memory so there is no reason not to materialise the result on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec3ae48-5799-4baf-92b2-e853b8b5bfd8",
   "metadata": {},
   "source": [
    "`persist()` is non-blocking so will return just as soon as _task graph optimisation_ (which is performed in the notebook kernel) is complete. Run the next cell and you will see it takes a few seconds to do _task graph optimisation_, and once that is complete the Jupyter notebook will be available for use again. At the same time the _dask dashboard_ will show tasks running as the result is computed and left on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0ed8f-8a01-4329-9998-d817622d6c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "on_cluster_result = ndvi_unweighted.persist()\n",
    "# wait(on_cluster_result)\n",
    "on_cluster_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a6d61-4bd8-42b6-8aaa-c9765a8083f6",
   "metadata": {},
   "source": [
    "The `on_cluster_result` will continue to show as a dask array on the cluster - not actual results. Think of it as a handle that links the _Jupyter client_ to the result on the _dask cluster_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744fd152-50d4-45f8-ab2e-7311aecb7548",
   "metadata": {},
   "source": [
    "The cluster will start the computation, but we can continue working in the notebook. Let's import a new visualization library: `hvplot`. We'll be using `datashader.rasterize` via `hvplot` to handle the visualisation of the full dataset which has many more pixels than what what is being displayed in the notebook. `hvplot.xarray` makes visualising `xarray` data a very natural experience, so the code is quite simple, a lot is taken care of for you.\n",
    "\n",
    "Notice also there are no bounds set on the dataset, we are viewing the entire result, including the _season_ dimension. `hvplot` will provide an interface for pan, zoom and season selection and you can use the mouse to move around the data.\n",
    "\n",
    ">__Tip:__ Keep watching your Dask dashboard to see how the calculations are progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e653ad4-155e-4bbe-86d5-fbc4e90149b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick calculation so the interactive UI is no more than 700 pixels wide and maintains aspect ratio.\n",
    "aspect = on_cluster_result.sizes['y']/on_cluster_result.sizes['x']\n",
    "width = 700\n",
    "height = int(width*aspect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728a39dd-ea5a-445b-b3c7-33f39d113dc8",
   "metadata": {},
   "source": [
    "The next cell will display the result - when its ready. The `rasterize` function will calculate a representative pixel for display from the full array on the dask cluster. If you monitor the dashboard you will see small bursts of activity across the workers and quite some waiting whilst data transfers occur to bring all the summary information back and transmit it to the Jupyter notebook. It's a large dataset, only the pixels you can see on the screen are sent to your web browser.\n",
    "\n",
    "You can use the controls on the right to pan and zoom around the full image. If you zoom in, `rasterize` will take a moment to generate a new summary for the current zoom level and show more or less detail. Similarly for panning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c7958-4bce-45dc-895f-255347f91c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.xarray \n",
    "import xarray as xr\n",
    "on_cluster_result.hvplot.image(groupby='season', rasterize=True).opts(\n",
    "        title=\"NDVI Seasonal Mean\",\n",
    "        cmap=\"RdYlGn\", # NDVI more green the larger the value. \n",
    "        clim=(-0.3, 0.8), # we'll clamp the range for visualisation to enhance the visualisation\n",
    "        colorbar=True,\n",
    "        frame_width=width,\n",
    "        frame_height=height\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcad62-a6e4-4504-9e41-d33f872ade7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Be a good dask user - Clean up the cluster resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acca97-686c-4d09-ba5b-a9aeede4bfba",
   "metadata": {},
   "source": [
    "Disconnecting your client is good practice, but the cluster will still be up so we need to shut it down as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578682e-917e-49b7-9f3e-88584a572936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144e53a-fb47-493f-9dbd-adef37566e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b02631-c13f-481b-bc1c-f183538e7f63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
