{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e374c84f-75e4-4859-bf4f-3a7847aef454",
   "metadata": {},
   "source": [
    "# Go Big or Go Home Part 1 - Dask fully distributed <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "In the previous notebooks we've been using `dask.distributed.LocalCluster` to split up our tasks and run them on the same compute node that is running the notebook. We noted that this could then use all cores to run tasks in parallel, greatly speeding up loading, and thanks to _chunks_ we can also process _some_ algorithms, like our NDVI seasonal mean, on datasets larger than available RAM.\n",
    "\n",
    "But what happens if your algorithm and dataset are such they cannot fit the compute nodes' RAM, or the result of the calculation is also massive, or its just so big (memory and computation) that it takes hours to compute?\n",
    "\n",
    "Well, `dask.distributed.LocalCluster` is just one member of the `dask.distributed` cluster family. There are several others but the one we will be using is Kubernetes Cluster (`KubeCluster`). Kubernetes is an excellent technology that takes advantage of modern Cloud Computing architectures to automatically provision (and remove) compute nodes on demand. It does a lot of other stuff well beyond the scope of this dask tutorial of course. The important point is that using `KubeCluster` we can dramatically expand the number of Compute Nodes to schedule our dask Workers to; potentially very dramatically.\n",
    "\n",
    "In this notebook we'll expand our NDVI seasonal mean calculation to cover all of Tasmania and take it back through two decades of observations. Along the way we'll explore the dask data structures, how computation proceeds, and what we can do to tune the performance. At this spatial size we'll also look at how to interactively visualise a result that is larger than the Jupyter notebook can handle.\n",
    "\n",
    "Everything we do next builds on the concepts of _chunks_, _tasks_, _data locality_, and _task graph_ covered previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59f381-17c5-49c1-9579-00a1a6f22e4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dask Gateway and remote dask schedulers\n",
    "\n",
    "Once we have multiple compute nodes to run workers on we have a lot more moving parts in the system. These parts are also fully distributed and will need to communicate between each other to pass results, perform tasks, confirm completion of tasks and ask for more work, etc. It is important to understand what the components are and how they interact because it can impact both _performance_ and _stablity_ of calculation, particularly at very large scales. In addition, _data locality_ really matters when your dataset is large - you don't want to `compute()` a 1 TB result and have it brought back to the Jupyter kernel on a 32 GiB machine! There are also subtleties to be aware of in how data gets from your Jupyter notebook to the dask distributed nodes - it has to be communicated somehow.\n",
    "\n",
    "This can all be a bit overwhelming to think about. Thankfully, you don't hit all of these at once as dask does good job of hiding many of the details but then remember our two \"laws\" of dask from the first notebook:\n",
    "1. The best thing about dask is it makes distributed parallel programming in the datacube easy\n",
    "1. The worst thing about dask is it makes distributed parallel programming in the datacube easy\n",
    "\n",
    "The transition point from gain to pain, and back to gain, is connected to these details. So let's define out various parts and their roles and start building our knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75e69d-237d-4411-bd88-59e2e7346826",
   "metadata": {},
   "source": [
    "### Kubernetes\n",
    "\n",
    "In a Kubernetes environment all programs that execute things run in __Pods__.  What a _pod_ is and how it works is a subject for another course and you can use an internet search to find out more. For our purposes it is sufficient to understand that the Jupyter notebook, the dask scheduler, the dask workers, and all the components that make this work are running in _Pods_.\n",
    "\n",
    "* _Pods_ have resources - memory, cpu, gpu, storage.\n",
    "* _Pods_ request resources and have resource limits.\n",
    "* _Pods_ communicate to each other over a network.\n",
    "* You can think of a _pod_ as being a kind of virtual PC on which you can run your programs.\n",
    "\n",
    "_Pods_ run on _Compute Nodes_ - physical hardware with an actual CPU, GPU, memory and storage. _Compute Nodes_ can run more than one _Pod_ so long as the sum of all the requests will fit. For example, if your _Compute Node_ has 64 GiB of RAM and your _worker pods_ request 14 GiB each, then 4 _workers Pods_ will run on 1 _Compute Node_.\n",
    "\n",
    "Thankfully, you don't need to figure out what Pods get placed where as Kubernetes will do that automatically. There is more to be said about this relationship and the impacts on performance and operational cost but for now just note that _Pods_ are where your code and data lives and they have requests and limits which you can control.\n",
    "\n",
    "This diagram shows a single user (_Joe_) running a Jupyter Notebook and connected to a single dask cluster with 5 Workers (running on 3 Worker Nodes).\n",
    "\n",
    "![Dask Cluster](../../resources/DaskCluster-ODCandDask.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c9853-fe04-439c-858f-4f2777eb4c48",
   "metadata": {},
   "source": [
    "The __Jupyter notebook__ is where you code is typed in. It has a Python kernel of its own and will be the __dask client__ that talks to the __dask cluster__. It is running in a Pod, as are all the components. So the Jupyter notebook is _separate_ from the other components in the system and communicates over a network.\n",
    "\n",
    "The __dask cluster__ is the __dask scheduler__ plus the group of __dask workers__ that process the tasks in a distributed manner. The _scheduler_ and the _workers_ are all Pods, which means they are _separate_ from each other and communicate over a network. This is different to `dask.distributed.LocalCluster` in which the Jupyter notebook, dask scheduler and workers all resided on the same machine and all communicated _very_ rapidly on the local machine's communications channel. Now they can be on entirely different _compute nodes_ and are _communicating over a much slower network_. We have the benefit of more compute resources, at the cost of slower communication.\n",
    "\n",
    "The __dask gateway__ is a new component and is used to manage __dask clusters__ (note that is plural). The __Jupyter notebook__ acts as a client to the _dask gateway_ and makes requests for a __dask cluster__ (both the _scheduler_ and the _workers_) to the _dask gateway_ to create and destroy them. The _dask gateway_ manages the lifecycle of the cluster on the user's behalf.\n",
    "\n",
    "> __Tip__: This means the dask clusters have an independent life cycle compared with the Jupyter notebook. Quitting your Jupyter notebook will not necessarily quit your dask cluster.\n",
    "\n",
    "Moreover, you can have more than one Jupyter notebook talking to the _same_ dask cluster simultaneously. There are some good reasons for doing this but we won't be touching on them in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4770317f-ec1b-4365-9a23-d4ef684a1a11",
   "metadata": {},
   "source": [
    "### Running our NDVI seasonal mean on the remote dask cluster\n",
    "\n",
    "Let's move our NDVI seasonal mean from the `LocalCluster` to our _dask gateway_ managed cluster, and add some extra compute resources to it.\n",
    "\n",
    "The biggest change here is simply how we start and shutdown the dask cluster. The rest of the code, to do the actual computation, is _exactly_ the same.\n",
    "\n",
    "The first thing we need to do is create a client so we can connect to the _dask gateway_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdc377-b388-44bc-ae3b-234848cd620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze the Gateway client\n",
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2529f3c-3b04-4bc0-a036-aab65a3602c9",
   "metadata": {},
   "source": [
    "Easy! We now have a `gateway` client variable. Using this we can start clusters, stop clusters, ask for a list of clusters we have running, set options for our scheduler and workers (like cpu and memory requests).\n",
    "\n",
    "Let's see what the `cluster_options` are. We don't need to guess, we can ask the `gateway`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fec8f2-91bf-495f-9493-21cfea00f3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway.cluster_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a08ee-c3aa-4b5a-bda8-20f6fc6b47b0",
   "metadata": {},
   "source": [
    "That's a lot I know. The majority of these don't need to change and most users will simply tweak _worker_ parameters:  _cores_, _threads_ (probably keeping it the same as _cores_), _memory_ and the _worker group_.\n",
    "\n",
    "We will be using the defaults for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da34ddb-f856-4234-9ef9-cf45095a83ba",
   "metadata": {},
   "source": [
    "### Create the Cluster\n",
    "\n",
    "Create the cluster with default options if it doesn't already exist. If a cluster exists in your namespace, the code below will connect to the first cluster. List the available clusters with `gateway.list_clusters()`.\n",
    "\n",
    "The cluster creation may take a little while (minutes) if a suitable _node_ isn't available for the _scheduler_. The same thing will occur for _workers_ when they start. If a _node_ does exist then this can happens in seconds.\n",
    "\n",
    "> __Tip__: Users are often confused by the changing start up time and think something is wrong.\n",
    "\n",
    "It can take _minutes_ for a brand new _node_ to be provisioned, please be patient. If it takes 10 minutes then yes, something is wrong and you should probably contact an administrator of the system if that problem persists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc86e6f-6afb-4a8b-83fd-48bed81bd142",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = gateway.list_clusters()\n",
    "if not clusters:\n",
    "    print('Creating new cluster. Please wait for this to finish.')\n",
    "    cluster = gateway.new_cluster()\n",
    "else:\n",
    "    print(f'An existing cluster was found. Connecting to: {clusters[0].name}')\n",
    "    cluster=gateway.connect(clusters[0].name)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7174b3-3e78-4496-9e09-7200aef5780b",
   "metadata": {},
   "source": [
    "### Scale the cluster\n",
    "\n",
    "Use the GatewayCluster widget (above) to adjust the cluster size. Alternatively use the cluster API methods.\n",
    "\n",
    "For many tasks 1 or 2 workers will be sufficient, although for larger areas or more complex tasks 5 to 10 workers may be used. If you are new to Dask, start with one worker and then scale your cluster if needed.\n",
    "\n",
    "In this notebook we'll start with 4 workers - that's 4x the resources for workers compared to our previous `LocalCluster`. In addition the _scheduler_ is also on its own node, and so is the Jupyter notebook kernel. Lots more resources for all the components involved.\n",
    "\n",
    "The next cell will use the cluster API to add 4 workers programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52629b33-88e4-4f2a-8388-9213e4808e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3edb753-4309-4196-b2f4-1d10dc0ea93b",
   "metadata": {},
   "source": [
    "### Connect to the cluster\n",
    "To connect to your cluster and start doing work, use the `get_client()` method. This step will wait until the workers are ready. You don't actually have to wait for the workers. The Jupyter notebook can be doing other things whilst the workers are coming up. We're waiting in this example so you don't end up with an unexpected wait later.\n",
    "\n",
    "***This may take a few minutes before your workers will be ready to use.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f3b60-0981-4090-bb29-a810dbdf15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()\n",
    "client.wait_for_workers(n_workers=4)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefb34f-b1fa-4774-92e9-836bbb6f21b6",
   "metadata": {},
   "source": [
    "The client widget provides a clickable _dask dashboard_ link so click that and you'll see your dashboard. It works the same as before despite the fact that everythign is now running in a distributed fashion. If you click the _Workers_ tab in the _dashboard_ you will see that we now have 32 cores (up from 8) made up of 4x 8-core workers. Lot's of RAM too.\n",
    "\n",
    "Go back to the _Status_, so you can watch everything run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62379a64-d3ae-43b9-ae09-20024b66e0f9",
   "metadata": {},
   "source": [
    "### Perform the computation - same as before\n",
    "\n",
    "We don't need to change any of our code to run this now, so let's repeat the full calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12b923-dc2e-4ebe-ad54-59ef7e1402e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.utils import masking\n",
    "\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e594cc-cfcb-41f2-b282-cefc14be13d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tasmania (near Little Pine Lagoon)\n",
    "central_lat = -42.019\n",
    "central_lon = 146.615\n",
    "\n",
    "# Set the buffer to load around the central coordinates\n",
    "# This is a radial distance for the bbox to actual area so bbox 2x buffer in both dimensions\n",
    "buffer = 0.8 ### This is the same size as the Larger than RAM example\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "# Data products - Landsat 8 ARD from Geoscience Australia\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "\n",
    "# Set the date range to load data over\n",
    "set_time = (\"2021-01-01\", \"2021-12-31\")\n",
    "\n",
    "# Set the measurements/bands to load. None eill load all of them\n",
    "measurements = None\n",
    "\n",
    "# Set the coordinate reference system and output resolution\n",
    "# This choice corresponds to Aussie Albers, with resolution in metres\n",
    "set_crs = \"epsg:3577\"\n",
    "set_resolution = (-30, 30)\n",
    "group_by = \"solar_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeb772-f4c2-4392-968b-5d5a6fd6f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1, \"x\":2048, \"y\":2048}, ## No change here, chunking spatially just like before\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset.nbytes / 2**30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3c928-43f0-4f53-8f10-6a64e0301c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "\n",
    "ndvi_unweighted = ndvi.groupby(\"time.season\").mean(\"time\")  # Calculate the seasonal mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39611d88-6e76-4ed6-a4da-fad1bc496287",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a501db5-d805-4b26-b511-ec2f8d169a52",
   "metadata": {},
   "source": [
    "As before there will be a short delay as the Jupyter kernel (_client_) is used to optimize the task graph before sending it to the _scheduler_ which will then execute _tasks_ on the _workers_.\n",
    "\n",
    "> __Tip__: You can open a terminal and use `htop` to monitor the Jupyter notebook CPU usage. You'll see at least one core using nearly 100% cpu usage during the optimisation phase. It will then drop back to idle as the _task graph_ is sent to the _scheduler_ at which point the dashboard will show activity on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9845f5-c3dd-4371-9a50-1fb4e8da2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_result.sel(season='DJF').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f463907-1920-4b3c-a33b-0d29467eefae",
   "metadata": {},
   "source": [
    "### Exploiting our new resources - adjusting our chunk size\n",
    "\n",
    "Before we \"Go Big\" let's take advantage of our new resources.\n",
    "\n",
    "We've gained memory. The more memory we have the more data we can operate on at once _and the less we need to communicate between nodes_. Communication over a network is slow relative to local communcation in a _pod_. So if we change the _chunking_ we may see an improvement in performance.\n",
    "\n",
    "_Chunking_ will also impact the number of tasks - the fewer chunks, the fewer tasks. This in turn will impact how much _task graph optimization_ is required, how hard the _scheduler_ has to work, and how much communication of partial results goes between workers (for example, passing the partial means around to get a final mean).\n",
    "\n",
    "Let's look at our existing chunk size - (1,2048,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d65191-67aa-4919-8b32-8b4762d02a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50815c1-fbc8-4853-9539-446e39abb3a7",
   "metadata": {},
   "source": [
    "8 MiB in use for `nbart_red` chunks. Of course this will vary with data type and stage of computation so when tuning dask you may need to check on the _chunk size_ and _tasks_ as your computation transforms the data. We're doing a simple computation here so we can focus on this initial value. With experience it does get easier to figure out when and where this parameter needs further adjustment. We'll look at some of this later.\n",
    "\n",
    "For now there are some things we can observe:\n",
    "1. 8 MiB is pretty small when our 4 workers have 32 Gigs each so we have room to grow even with all the temporaries to allow for.\n",
    "   * _We should monitor the worker memory usage in the dashboard as we make changes to ensure none are spilling to disk as that will slow things down and is unnecessary in this case_\n",
    "1. Geospatial operations - the data load, the reprojection, even the masking - may benefit from having a larger spatial area.\n",
    "   * No point going too large though as the satellite paths have a finite width and we'll just have lots of empty space.\n",
    "1. The computation involves a seasonal mean, which means some temporal grouping might improve performance.\n",
    "   * That said, _chunks_ are a unit for communication and it may mean that we're passing more information around than is necessary if we group too much together across the seasonal boundaries.\n",
    "\n",
    "So we have good reason to increase our chunks both spatially and temporally; just be mindful of the impact on communication of results between nodes. The mean is seasonal and Landsat 8 performs repeat passes nominally every 16 days, so let's do a small grouping in time. We'll also increase the spatial chunking slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc9f88-7b72-48e0-a1e3-700273a85cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":2, \"x\":3072, \"y\":3072},\n",
    "            group_by=group_by,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872da0b0-9a9f-454f-81e7-eac5cd2e3306",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a323f5-fc2a-4167-925e-ecd607812bd7",
   "metadata": {},
   "source": [
    "As you can see the number of tasks has dropped and our chunks are larger at 36 MiB.\n",
    "\n",
    "There is a small slither along the bottom because the chunk size isn't a good fit for the actual array size: `slither = 6253 - (2 x 3072) = 109`. Let's expand our chunk size in that direction slightly to give us a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e122f-78eb-4dc7-ab66-164256d4bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":2, \"x\":3072, \"y\":3127},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset.nbart_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809561da-7fe9-4a0e-8ec8-0f7ee28564a0",
   "metadata": {},
   "source": [
    "Notice how that small change to remove the sliver only marginally increased our chunk memory usage (by .64 MiB) but dramatically reduced the number of chunks (down from 210 to 140).\n",
    "\n",
    "Let's see what this does to our performance. We need to re-run the code to update all the intermediate variables in our calculation and call `compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec7285-fa1d-479c-8bf8-b58b2785195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "\n",
    "ndvi_unweighted = ndvi.groupby(\"time.season\").mean(\"time\")  # Calculate the seasonal mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818d321-3019-4ef2-a965-cd99deed996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08376e4-2473-4f63-a80e-da65347a1d8e",
   "metadata": {},
   "source": [
    "You will notice the time for _task graph optimization_ - the delay between executing the cell above and seeing processing in the cluster dashboard is down significantly. Fewer tasks means less time in optimization.\n",
    "\n",
    "We've significant decreased the computation time as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f110567-d864-4891-8953-58d2ec4c5bc5",
   "metadata": {},
   "source": [
    "There is one more thing we can do before we \"Go Big\". We've done this before and its simple enough. Save dask the challenge of figuring out which measurements we aren't using by telling it only to load the ones we do use.\n",
    "Let's add out measurements list in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaac669-c816-44df-b10a-d00f3345567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurements = [ \"oa_fmask\", \"nbart_red\", \"nbart_nir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b423b9-a28e-48e2-81c5-b2524802fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":2, \"x\":3072, \"y\":3127},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "\n",
    "ndvi_unweighted = ndvi.groupby(\"time.season\").mean(\"time\") # perform the seasonal mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8c1f7-99dd-4907-8cda-b268bb371623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13f71e8-63bd-4fc6-b6a4-cb95109377aa",
   "metadata": {},
   "source": [
    "This didn't make to much difference to computational time but it has shortened the _task graph optimisation_ phase a little more. That time isn't prohibitive in this example but as we \"Go Big\" it will be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfcad62-a6e4-4504-9e41-d33f872ade7a",
   "metadata": {},
   "source": [
    "# Be a good dask user - Clean up the cluster resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acca97-686c-4d09-ba5b-a9aeede4bfba",
   "metadata": {},
   "source": [
    "Disconnecting your client is good practice, but the cluster will still be up so we need to shut it down as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578682e-917e-49b7-9f3e-88584a572936",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc639b8-3b1f-475d-a3f6-14280c038399",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02252951-2085-40ef-80af-4433a0d34d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
