{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54240a84-8659-4d34-af38-1249129a221a",
   "metadata": {},
   "source": [
    "# Dask Local Cluster - Larger than memory computation <img align=\"right\" src=\"../../resources/csiro_easi_logo.png\">\n",
    "\n",
    "In the ODC and Dask (LocalCluster) notebook we saw how dask can be used to speed up IO and computation by parallelising operations into _chunks_ and _tasks_, and using _delayed tasks_ and _task graph_ optimization to remove redundant tasks when results are not used.\n",
    "\n",
    "Using _chunks_ provides one additional capability beyond parallelisation - _the ability to perform computations that are larger than available memory_.\n",
    "\n",
    "Since dask operations are performed on _chunks_ it is possible for dask to perform operations on smaller pieces that each fit into memory. This is particularly useful if you have a large amount of data that is being reduced, say by performing a seasonal mean.\n",
    "\n",
    "As with parallelisation, not all algorithms are amenable to being broken into smaller pieces so this won't always be possible. Dask arrays though go a long way to make this easier for a great many operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d810c6f-46aa-48b0-a7a9-82151b29bb96",
   "metadata": {},
   "source": [
    "Firstly, some initial imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bf42b0-b251-4d68-9af0-cf9ab988c886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EASI tools\n",
    "import git\n",
    "import sys, os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "if repo.working_tree_dir not in sys.path: sys.path.append(repo.working_tree_dir)\n",
    "from easi_tools import EasiNotebooks, notebook_utils\n",
    "easi = EasiNotebooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ae885-c6bb-4d90-8769-e9a41d8b92cb",
   "metadata": {},
   "source": [
    "We'll continue using the same algorithm as before but this time we're going to modify it's memory usage to exceed the LocalCluster's available memory. This example notebook is setup to run on a compute node with 28 GiB of available memory and 8 cores for the LocalCluster. We'll make that explicit here in case you are blessed with a larger number of resources.\n",
    "\n",
    "Let's start the cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608053d-83db-45fe-9b09-9ea089edaf37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=4)\n",
    "cluster.scale(n=2, memory=\"14GiB\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fa086-3587-4717-9466-8eb3ca6a7fcb",
   "metadata": {},
   "source": [
    "We can monitor memory usage on the workers using the dask dashboard URL below and the Status tab. The workers are local so this will be memory on the same compute node that Jupyter is running in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303b0e2-abd8-475c-ae4c-14de9e7a3c3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dashboard_address = notebook_utils.localcluster_dashboard(client=client,server=easi.hub)\n",
    "print(dashboard_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bbfd77-faf4-4834-99ef-7688911e02aa",
   "metadata": {},
   "source": [
    "---\n",
    "As we will be using __Requester Pays__ buckets in AWS S3, we need to run the `configure_s3_access()` function below with the `client` option to ensure that Jupyter and the cluster have the correct permissions to be able to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb63d8-7dc5-48e2-a5f8-8d65b5cacf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.utils.aws import configure_s3_access\n",
    "configure_s3_access(aws_unsigned=False, requester_pays=True, client=client);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc032bf6-2105-4237-9121-0f557c7f65a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.utils import masking\n",
    "\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026408c9-f44c-4270-b7fa-325646f22d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the centroid of the coordinates in the default configuration\n",
    "central_lat = sum(easi.latitude)/2\n",
    "central_lon = sum(easi.longitude)/2\n",
    "\n",
    "# or set your own by uncommenting and editing the following lines\n",
    "# central_lat = -42.019\n",
    "# central_lon = 146.615\n",
    "\n",
    "# Set the buffer to load around the central coordinates\n",
    "# This is a radial distance for the bbox to actual area so bbox 2x buffer in both dimensions\n",
    "buffer = 0.05\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "# Data products - Landsat 8 ARD from Geoscience Australia\n",
    "products = easi.product('landsat')\n",
    "\n",
    "# Set the date range to load data over \n",
    "set_time = (\"2021-01-01\", \"2021-12-31\")\n",
    "\n",
    "# Set the measurements/bands to load. None eill load all of them\n",
    "measurements = None\n",
    "\n",
    "# Set the coordinate reference system and output resolution\n",
    "set_crs = easi.crs('landsat')  # If defined, else None\n",
    "set_resolution = easi.resolution('landsat')  # If defined, else None\n",
    "# set_crs = \"epsg:3577\"\n",
    "# set_resolution = (-30, 30)\n",
    "\n",
    "group_by = \"solar_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52fb17-cd58-439a-921e-775ce94ebcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd8e21-cdae-4ee2-a19e-5803617e85bf",
   "metadata": {},
   "source": [
    "We can check the total size of the dataset using `nbytes`. We'll divide by 2**30 to have the result display in [gibibytes](https://simple.wikipedia.org/wiki/Gibibyte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f1009-9734-4bb0-b49a-4918af09af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset size (GiB) {dataset.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a30454-290b-4f1f-a8ca-32c697b68b89",
   "metadata": {},
   "source": [
    "As you can see this ROI and spatial range (1 year) is tiny, let's scale up by increasing our ROI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a93b-a050-4d7c-bd50-fc83d35c1126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "buffer = 1\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b66c21-f5e8-48de-bb6e-1ce34f885bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"qa_pixel\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "print(f\"dataset size (GiB) {dataset.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb169a-7fee-4703-8203-cc871e3b2bcc",
   "metadata": {},
   "source": [
    "Okay, this is now larger than the available memory that our Jupyter node has available (which you should be able to see at the bottom of your window - probably 29.00 GB). This creates issues for calculation. We need to have a solution that lets us calculate the information that we want without the machine running out of memory. \n",
    "\n",
    "Dask can compute many tasks and handle large amounts of data over the course of a series of calculations. Collectively, these calculations might work on more data in total than can fit in RAM, but it is a problem if the final product is too big to fit in RAM. Below we will change the dataset so that the final result can fit in RAM and then use the `.compute()` function to run all the calculations.\n",
    "\n",
    "Let's take a look at the memory usage for one of the bands, we'll use `red`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891babf5-678a-4a40-9502-2418b4820e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd4c1c-647a-4a98-8af0-f10ef5f029d0",
   "metadata": {},
   "source": [
    "You can see the year now has more time observations than in the first dataset because we've expanded the area of interest and picked up multiple satellite passes. The spatial dimensions are also much larger.\n",
    "\n",
    "Take a note of the _Chunk Bytes_ - probably around 80 MiB. This is the smallest unit of this dataset that dask will do work on. To do an NDVI calculation, dask will need two bands, the mask, the result and a few other temporary variables in memory at once. This means whilst this value is an indicator of memory required on a worker to perform an operation it is not the total, which will depend on the operation.\n",
    "\n",
    "We can adjust the amount of memory per chunk further by _chunking_ the spatial dimension. Let's split it into 2048x2048 size pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dfc58-4c65-46e9-81b5-80aba1db87b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1, \"x\":2048, \"y\":2048},  ## Adjust the chunking spatially as well\n",
    "            group_by=group_by,\n",
    "        )\n",
    "print(f\"dataset size (GiB) {dataset.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da60763-1813-43fa-857c-99153c99a6e5",
   "metadata": {},
   "source": [
    "As you can see the total dataset size stays the same. \n",
    "\n",
    "Look at the `red` data variable below. You can see the chunk size has reduced to 8 MiB, and there are now more chunks (around 700-800) - compared with around 60 previously. This will result in a higher number of Tasks for Dask to work on. This makes sense: smaller chunks, more tasks.\n",
    "\n",
    "> __TIP__: The _relationship between tasks and chunks_ is a critical tuning parameter.\n",
    "\n",
    "Workers have limits in memory and compute capacity. The Dask Scheduler has limits in how many tasks it can manage efficiently (and remember it is tracking all of the data variables, not just this one). The trick with Dask is to give it a good number of chunks of data which aren't too big and don't result in too many tasks. There is always a trade-off and each calculation will be different. Ideally, you want chunks to be aligned with how the data is stored, or how the data is going to be used. If those two things are different, then rechunking can result in large amounts of data needing to be held in the cluster memory, which could result in failures. In the same way, if chunks are too large, they might end up taking up too much memory, causing a crash. This is sometimes down to trial and error.\n",
    "\n",
    "Later, when we move to a fully remote and distributed cluster, _chunks_ also become an important element in communicating between workers over networks.\n",
    "\n",
    "If you look carefully at the cube-like diagram in the summary below you will see that some internal lines showing the chunk boundaries for the spatial dimensions. 2048 wasn't an even multiplier so dask has made some chunks on the edges smaller. The specification of `chunks` is a guide: the actual data, numpy arrays in this case, are made into `chunk` sized shapes or smaller. These are called `blocks` in dask and represent the actual shape of the numpy array that will be processed.\n",
    "\n",
    "Somewhat confusingly the terms `blocks` and `chunks` are also used in dask literature and you'll need to check the context to see if it is referring to the _specification_ or the _actual block of data_. For the moment this differentiation doesn't matter but when performing low level custom operations knowing that your `blocks` might be a different shape does matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b96e79-6c54-4f03-91a8-6b4a840d3225",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa1249-f0af-4e83-a356-31cfc34e1c1c",
   "metadata": {},
   "source": [
    "We won't worry to much about tuning these parameters right now and instead will focus on processing this larger dataset. As before we can exploit dask's ability to use _delayed_ tasks and apply our masking and NDVI directly to the full dataset. We'll also add an unweighted seasonal mean calculation using `groupby(\"time.season\").mean(\"time\")`. Dask will seek to complete the reductions (by chunk) first as they reduce memory usage.\n",
    "\n",
    "It's probably worth monitoring the dask cluster memory usage via the dashboard _Workers Memory_ to see just how little ram is actually used during this calculation despite it being performed on a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258f835-1f34-4dc1-8d99-e89d6233528f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dashboard_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b1aba-85d4-415d-84e3-745fb38138a2",
   "metadata": {},
   "source": [
    "---\n",
    "We will now calculate NDVI and group the results by season:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e9923-6884-40d1-a53a-2babfae82a66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify pixels that don't have cloud, cloud shadow or water\n",
    "from datacube.utils import masking\n",
    "\n",
    "good_pixel_flags = {\n",
    "    'nodata': False,\n",
    "    'cloud': 'not_high_confidence',\n",
    "    'cloud_shadow': 'not_high_confidence',\n",
    "    'water': 'land_or_cloud'\n",
    "}\n",
    "\n",
    "cloud_free_mask = masking.make_mask(dataset['qa_pixel'], **good_pixel_flags)\n",
    "\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nir08 - cloud_free.red\n",
    "band_sum = cloud_free.nir08 + cloud_free.red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "\n",
    "ndvi_unweighted = ndvi.groupby(\"time.season\").mean(\"time\")  # Calculate the seasonal mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bfe1d-799c-4449-9b80-19bcc7f99061",
   "metadata": {},
   "source": [
    "Let's check the shape of our result - it should have 4 seasons now instead of the individual dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009724fc-3c43-4225-9404-85b01f94d1fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ndvi_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88af8e-beac-4779-b07b-5b001c5505bb",
   "metadata": {},
   "source": [
    "Before we do the `compute()` to get our result we should make sure the final result will fit in memory for the Jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced41b22-710e-4603-8f04-49c7f8849bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dataset size (GiB) {ndvi_unweighted.nbytes / 2**30:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb1346-242d-412e-b824-1ae6a6b1b2e9",
   "metadata": {},
   "source": [
    "This shows that the resulting data should be around 1 GiB of data, which will fit in local memory.\n",
    "\n",
    "If you are monitoring the cluster when you run the cell below, you might notice a delay between running the next cell and actual computation occuring. Dask performs a _task graph optimisation_ step on the _client_ not the cluster. How long this takes depends on the number of tasks and complexity of the graph. The speed of this step has improved recently due to recent Dask updates. We'll talk more about this later.\n",
    "\n",
    "In the meantime, run the next cell and watch dask compute the result without running out of memory. You might notice that your cluster spills some data to disk (the grey part of the bars in the _Bytes stored per worker_ graph). This is not normally desirable and slows down the calculation (because reading and writing to/from the disk is slower than to/from RAM), but it is a mechanism used by Dask to help manage large calculations. \n",
    "\n",
    ">__Tip:__ don't forget to look at your Dask Dashboard (URL a few cells above) to watch what is happening in your cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3915b-1ccc-4ece-9559-cd27295a14f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e6946-4a12-4d69-bf0a-3f02d64e2371",
   "metadata": {},
   "source": [
    "To avoid northern/southern hemisphere differences, the `season` values are represented as acronyms of the months that make them up, so:\n",
    "- December, January, February = DJF\n",
    "- March, April, May = MAM\n",
    "- June, July, August = JJA\n",
    "- September, October, November = SON\n",
    "\n",
    "Let's plot the result for `DJF`. This will take a few seconds, the image is several thousand pixels across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cfdb6-c5c7-4468-a379-9094a0003b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actual_result.sel(season='DJF').plot(robust=True, size=6, aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757725f-c923-403a-8943-d79bb42dafc9",
   "metadata": {},
   "source": [
    "Not the most useful visualisation as a small image, and a little slow. Dask can help with this too but that's a topic for another notebook. There are many other ways to work with Dask and optimize performance. This is just the beginning of how to manage large calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75730dc0-0940-44e2-b656-747f6862915d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Be a good dask user - Clean up the cluster resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeb0ee-6888-4362-8615-19cf217e80c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d22aa2-f239-4a0a-8b8a-331ca32b09d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
