{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54240a84-8659-4d34-af38-1249129a221a",
   "metadata": {},
   "source": [
    "# Dask Local Cluster - Larger than memory computation <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "In the ODC and Dask (LocalCluster) notebook we saw how dask can be used to speed up IO and computation by parallelising operations into _chunks_ and _tasks_, and using _delayed tasks_ and _task graph_ optimization to remove redundant tasks when results are not used.\n",
    "\n",
    "Using _chunks_ provides one additional capability beyond parallelisation - _the ability to perform computations that are larger than available memory_.\n",
    "\n",
    "Since dask operations are performed on _chunks_ it is possible for dask to perform operations on smaller pieces that each fit into memory. This is particularly useful if you have a large amount of data that is being reduced, say by performing a seasonal mean.\n",
    "\n",
    "As with parallelisation, not all algorithms are amenable to being broken into smaller pieces so this won't always be possible. Dask arrays though go a long way to make this easier for a great many operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ae885-c6bb-4d90-8769-e9a41d8b92cb",
   "metadata": {},
   "source": [
    "We'll continue using the same algorithm as before but this time we're going to modify it's memory usage to exceed the LocalCluster's available memory. This example notebook is setup to run on a compute node with 28 GiB of available memory and 8 cores for the LocalCluster. We'll make that explicit here in case you are blessed with a larger number of resources.\n",
    "\n",
    "Let's start the cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2608053d-83db-45fe-9b09-9ea089edaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(n_workers=2, threads_per_worker=4)\n",
    "cluster.scale(n=2, memory=\"14GiB\")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69fa086-3587-4717-9466-8eb3ca6a7fcb",
   "metadata": {},
   "source": [
    "We can monitor memory usage on the workers using the dask dashboard and the Status tab. The workers are local so this will be memory on the same compute node that Jupyter is running in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303b0e2-abd8-475c-ae4c-14de9e7a3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.environ.get(\"JUPYTERHUB_USER\")\n",
    "dashboard_address=f'https://hub.csiro.easi-eo.solutions/user/{user}/proxy/8787/status'\n",
    "print(dashboard_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc032bf6-2105-4237-9121-0f557c7f65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.utils import masking\n",
    "\n",
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026408c9-f44c-4270-b7fa-325646f22d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tasmania (near Little Pine Lagoon)\n",
    "central_lat = -42.019\n",
    "central_lon = 146.615\n",
    "\n",
    "# Set the buffer to load around the central coordinates\n",
    "# This is a radial distance for the bbox to actual area so bbox 2x buffer in both dimensions\n",
    "buffer = 0.05\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "# Data products - Landsat 8 ARD from Geoscience Australia\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "\n",
    "# Set the date range to load data over \n",
    "set_time = (\"2021-01-01\", \"2021-12-31\")\n",
    "\n",
    "# Set the measurements/bands to load. None eill load all of them\n",
    "measurements = None\n",
    "\n",
    "# Set the coordinate reference system and output resolution\n",
    "# This choice corresponds to Aussie Albers, with resolution in metres\n",
    "set_crs = \"epsg:3577\"\n",
    "set_resolution = (-30, 30)\n",
    "group_by = \"solar_day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52fb17-cd58-439a-921e-775ce94ebcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd8e21-cdae-4ee2-a19e-5803617e85bf",
   "metadata": {},
   "source": [
    "We can check the total size of the dataset using `nbytes`. We'll divide by 2**30 to have the result display in [gibibytes](https://simple.wikipedia.org/wiki/Gibibyte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f1009-9734-4bb0-b49a-4918af09af8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbytes / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a30454-290b-4f1f-a8ca-32c697b68b89",
   "metadata": {},
   "source": [
    "As you can see this ROI and spatial range (1 year) is tiny, let's scale up by increasing our ROI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53a93b-a050-4d7c-bd50-fc83d35c1126",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 0.8\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b66c21-f5e8-48de-bb6e-1ce34f885bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1},\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset.nbytes / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb169a-7fee-4703-8203-cc871e3b2bcc",
   "metadata": {},
   "source": [
    "Okay, larger than available memory.\n",
    "\n",
    "Let's take a look at the memory usage for one of the bands, we'll use `nbart_red`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891babf5-678a-4a40-9502-2418b4820e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd4c1c-647a-4a98-8af0-f10ef5f029d0",
   "metadata": {},
   "source": [
    "You can see the year now has more time observations (69) because we've expanded the ROI and picked up multiple satellite passes. The spatial dimensions are also much larger.\n",
    "\n",
    "Take a note of the _Chunk Bytes_ - 61.58 MiB. This is the smallest unit of this dataset that dask will work on. To do an NDVI calculation, dask will need two bands, the mask, the result and a few temporaries in memory at once. This means whilst this value is an indicator of memory required on a worker to perform an operation it is not the total, which will depend on the operation.\n",
    "\n",
    "We can adjust the amount of memory per chunk further by _chunking_ the spatial dimension. Let's split it into 2048x2048 size pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497dfc58-4c65-46e9-81b5-80aba1db87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks =  {\"time\":1, \"x\":2048, \"y\":2048},  ## Adjust the chunking spatially as well\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset.nbytes / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da60763-1813-43fa-857c-99153c99a6e5",
   "metadata": {},
   "source": [
    "As you can see the total dataset size stays the same. \n",
    "\n",
    "Look at the `nbart_red` data variable. You can see the chunk size has reduced to 8 MiB, and there are now 828 chunks - compared with 69 previously. The number of Tasks has increased proportionately too. This makes sense: smaller chunks, more tasks.\n",
    "\n",
    "> __TIP__: The _relationship between tasks and chunks_ is a critical tuning parameter.\n",
    "\n",
    "Workers have limits in memory and compute capacity. The Dask Scheduler has limits in how many tasks it can manage efficiently (and remember it is tracking all of the data variables, not just this one). Later, when we move to a fully remote and distributed cluster, _chunks_ also become an important element in communicating between workers over networks.\n",
    "\n",
    "If you look carefully at the figure you will see that some internal lines showing the chunk boundaries for the spatial dimensions. 2048 wasn't an even multiplier so dask has made these ones smaller. The specification of `chunks` is a guide: the actual data, numpy arrays in this case, are made into `chunk` sized shapes or smaller. These are called `blocks` in dask and represent the actual shape of the numpy array that will be processed.\n",
    "\n",
    "Somewhat confusingly the terms `blocks` and `chunks` are also used in dask literature and you'll need to check the context to see if it is referring to the _specification_ or the _actual block of data_. For the moment this differentiation doesn't matter but when performing low level custom operations knowing that your `blocks` might be a different shape does matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b96e79-6c54-4f03-91a8-6b4a840d3225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa1249-f0af-4e83-a356-31cfc34e1c1c",
   "metadata": {},
   "source": [
    "We won't worry to much about tuning these parameters right now and instead will focus on processing this 130 GiB dataset. As before we can exploit dask's ability to use _delayed_ tasks and apply our masking and NDVI directly to the 130 GiB dataset. We'll also add an unweighted seasonal mean calculation using `groupby(\"time.season\").mean(\"time\")`. Dask will seek to complete the reductions (by chunk) first as they reduce memory usage.\n",
    "\n",
    "It's probably worth monitoring the dask cluster memory usage via the dashboard _Workers Memory_ to see just how little ram is actually used during this calculation despite it being performed on a 130 GiB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6258f835-1f34-4dc1-8d99-e89d6233528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dashboard_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e9923-6884-40d1-a53a-2babfae82a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "\n",
    "ndvi_unweighted = ndvi.groupby(\"time.season\").mean(\"time\")  # Calculate the seasonal mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874bfe1d-799c-4449-9b80-19bcc7f99061",
   "metadata": {},
   "source": [
    "Let's check the shape of our result - it should have 4 seasons now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009724fc-3c43-4225-9404-85b01f94d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_unweighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88af8e-beac-4779-b07b-5b001c5505bb",
   "metadata": {},
   "source": [
    "Before we do the `compute()` to get our result we should make sure the final result will fit in memory for the Jupyter kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced41b22-710e-4603-8f04-49c7f8849bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_unweighted.nbytes  / 2**30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcb1346-242d-412e-b824-1ae6a6b1b2e9",
   "metadata": {},
   "source": [
    "From 130 GiB down to < 1 Gig for the result.\n",
    "\n",
    "If you are monitoring the cluster at this point you will notice a delay between running the next cell and actual computation occuring. Dask performs a _task graph optimisation_ step on the _client_ not the cluster. How long this takes depends on the number of tasks and complexity of the graph. We'll talk more about this later.\n",
    "\n",
    "In the meantime, run the next cell and watch dask compute the result without running out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e1b3b-1a9b-48b9-9a94-acdf2e4db764",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_result = ndvi_unweighted.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e6946-4a12-4d69-bf0a-3f02d64e2371",
   "metadata": {},
   "source": [
    "Let's plot the result for the summer (DJF). This will take a few seconds, the image is several thousand pixels across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cfdb6-c5c7-4468-a379-9094a0003b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_result.sel(season='DJF').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757725f-c923-403a-8943-d79bb42dafc9",
   "metadata": {},
   "source": [
    "Not the most useful visualisation as a thumbnail, and a little sluggish. Dask can help with this too but that's a topic for another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75730dc0-0940-44e2-b656-747f6862915d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Be a good dask user - Clean up the cluster resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeb0ee-6888-4362-8615-19cf217e80c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d22aa2-f239-4a0a-8b8a-331ca32b09d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
