{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e59cba-16e4-49b9-928e-e41d1c0196f6",
   "metadata": {},
   "source": [
    "# ODC and Dask (LocalCluster) <img align=\"right\" src=\"../resources/csiro_easi_logo.png\">\n",
    "\n",
    "This notebook explores the use of ODC with Dask LocalCluster. The goal is to introduce fundamental concepts and the role Dask can serve with `datacube` and subsequent computation using `xarray`.\n",
    "\n",
    "The example computation is fairly typical of an EO data processing pipeline. We'll be using a small area and time period to start with and progressively scaling this example. EO scientists may find some aspects of these examples unrealistic, but this isn't an EO science course. :-). \n",
    "\n",
    "For the base example we'll be using the Australian island state of Tasmania as our Region of Interest (ROI). Intially a paddock size, and progressively increasing to the entire island.\n",
    "The basic algorithm is:\n",
    "  1. Specify Region of Interest, Satellite products, EO satellite bands, Time range, desired CRS for the `datacube` query\n",
    "  1. Load data using `datacube.load()`\n",
    "  1. Mask valid data\n",
    "  1. Visualisation of the ROI\n",
    "  1. Compute NDVI\n",
    "  1. Visualise NDVI\n",
    "  \n",
    "  \n",
    "__Some cells in this notebook will take minutes to run so be patient__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e55d3-3998-4b91-bf52-0aad8bb9180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube\n",
    "from datacube.utils import masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aca1bd-ff2c-4d33-8a3b-6ca520f017d6",
   "metadata": {},
   "source": [
    "The next cell sets out all the query parameters used in our `datacube.load()`.\n",
    "For this run we keep the ROI quite small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b679f-4927-4cef-9af8-ae94ed5c0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Tasmania (near Little Pine Lagoon)\n",
    "central_lat = -42.019\n",
    "central_lon = 146.615\n",
    "\n",
    "# Set the buffer to load around the central coordinates\n",
    "# This is a radial distance for the bbox to actual area so bbox 2x buffer in both dimensions\n",
    "buffer = 0.05\n",
    "\n",
    "# Compute the bounding box for the study area\n",
    "study_area_lat = (central_lat - buffer, central_lat + buffer)\n",
    "study_area_lon = (central_lon - buffer, central_lon + buffer)\n",
    "\n",
    "# Data products - Landsat 8 ARD from Geoscience Australia\n",
    "products = [\"ga_ls8c_ard_3\"]\n",
    "\n",
    "# Set the date range to load data over - just a month for the moment\n",
    "set_time = (\"2021-01-01\", \"2021-01-31\")\n",
    "\n",
    "# Set the measurements/bands to load. None will load all of them\n",
    "measurements = None\n",
    "\n",
    "# Set the coordinate reference system and output resolution\n",
    "# This choice corresponds to Australian Albers, with resolution in metres\n",
    "set_crs = \"epsg:3577\"\n",
    "set_resolution = (-30, 30)\n",
    "group_by = \"solar_day\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd89d20-7532-45e8-85bb-6a29bd078971",
   "metadata": {},
   "source": [
    "Now initialise the `datacube`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e72951b-e95c-4901-84df-b8c28bf9fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ec0c7-84d8-4a66-8d89-6c54e291e2e7",
   "metadata": {},
   "source": [
    "Now load the data. We use `%%time` to keep track of how long things take to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8bdfa2-7422-41e2-91e0-3ca7ca630ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            group_by=group_by,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ec342-d654-41c1-bce7-c91b84269019",
   "metadata": {},
   "source": [
    "The result of the `datacube.load()` function is an `xarray.Dataset`. The notebook can be used to render a description of the `dataset` variable as an html block with a _lot of useful information_ about the structure of data.\n",
    "If you open up the `Data variables` (click the > Data variables) and click on the stacked cylinders for one of them (nbart_red, nbart_green, ...)  you will see the actual data array is available and shown in summary form.\n",
    "\n",
    "This visualisation will become increasingly importantly when dask is enabled and as scale out occurs so take a moment now to just poke around the interface.\n",
    "Notice that at this stage we have 5 data variables, 4 time observations and each observation is y:391, by x:323 pixels (30 m pixels). We're at _paddock scale_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b096d-0659-43d1-91fb-e4499e8522f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800f9f2-ef28-4bf0-94bb-0587c2c0b181",
   "metadata": {},
   "source": [
    "Next up filter out everything that isn't an `fmask:valid` pixel and compute the NDVI. Since we aren't specifying a time range this will be performed for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ccfa8f-6ed5-470c-9802-998ca7839d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce1fad-4110-4585-9efd-acf271c70fc5",
   "metadata": {},
   "source": [
    "The result `ndvi` is an `xarray.DataArray`. Let's take a look at it. Again the notebook will render an html version of the data in summary form.\n",
    "Notice again the actual data values are being shown and that there are 4 time slices and the shape is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa68d03-b2a5-4124-88f6-3f8a9e8360cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7596db-6a59-42a8-a8fd-411c909d4244",
   "metadata": {},
   "source": [
    "Raw numbers aren't nice to look at so let's draw a time slice. We'll select just one of them to draw and pick one that didn't get masked out by cloud completely. The masked out white bit is _Little Pine Lagoon_, a water body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2905d2-042b-4751-83bd-55318aea96ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.isel(time=1).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb76402-0712-4acc-9d32-1adb9f351c1c",
   "metadata": {},
   "source": [
    "# Exploring Dask with the ODC - Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb7e93a-c72d-4d85-9d84-249beac08553",
   "metadata": {},
   "source": [
    "Let's set our time range to a couple of weeks, or approximately two passes of Landsat 8 for this ROI. Less data will allow us to explore how dask works with the `datacube` and `xarray` libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5270b-2c88-4d6b-96ca-4a1b12c99adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_time = (\"2021-01-01\", \"2021-01-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f1e62-1ab2-459e-b6fa-3f564ac13209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d1061-f60b-4ead-98c3-52d852731716",
   "metadata": {},
   "source": [
    "As before you can see the actual data in the results but this time there are only 2 observation times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e104c-9427-4de1-9146-9a6067c55dcf",
   "metadata": {},
   "source": [
    "Now let's create a `LocalCluster` as we did in the earlier notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351ebf0-5c3b-4fbd-ae3e-a27f008b6b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d10fbc-951b-40ad-a86e-c2e1ccc812ee",
   "metadata": {},
   "source": [
    "You may like to open up the dashboard for the cluster, although for this notebook we won't be talking about the dashboard (that's for a later discussion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c265cddb-1371-4080-9eb2-4c0962b4242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "user = os.environ.get(\"JUPYTERHUB_USER\")\n",
    "dashboard_address=f'https://hub.csiro.easi-eo.solutions/user/{user}/proxy/8787/status'\n",
    "print(dashboard_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39f921-db05-4466-bf51-70c3e19eb602",
   "metadata": {},
   "source": [
    "`datacube.load()` will use the default `dask` cluster (the one we just created) if the `dask_chunks` parameter is specified.\n",
    "\n",
    "The chunk shape and memory size is a critial parameter in tuning `dask` and we will be discussing it in great detail as scale increases. For now we're simply going to specify that the `time` dimension should individually chunked (`1` slice of time) and by not specifying any chunking for the other dimensions they will be form a single contiguous block.\n",
    "\n",
    "If that made no sense what's so ever, that's fine because we will look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abfacab-bd47-4e78-b841-233d054309ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"time\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9603c34-b948-4e29-b0a8-0d4ae1831243",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, ###### THIS IS THE ONLY LINE CHANGED. #####\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa0f4d-0b20-4b0d-b3ed-7ea46d33497b",
   "metadata": {},
   "source": [
    "First thing you probably noticed is that whilst only one line changed the load time dropped to sub-seconds!\n",
    "The second thing you probably noticed is if you poked around the `data variables` as before there is no data but a nice diagram. It's really fast because it didn't do anything!\n",
    "\n",
    "When `datatcube` has `dask_chunks` specified it switches to creating `xarrays` with `dask.arrays` in the backend and `lazy loads` them - no data is loaded until used. If you look at one of the data variables you will see it now has `dask.array<....>` rather than values and the cylinder icon will show the Array _and_ Chunk parameters, not actual data.\n",
    "\n",
    "The `datacube.load()` has used the `dask.Delayed` interface which will not perform any `tasks` until the _result_ of the `task` is actually required. We'll load the data in a moment but first let's take a look at the parameters in that pretty visualisation. Click on the cylinder for the `red` Data variables and look at the table and the figure. You can see that:\n",
    "  1. The Array is `493.33 kiB` in total size and is broken into Chunks which have size `246.67 kiB`\n",
    "  2. The Array shape is `(2, 391, 323) (time, y, x)` but each chunk is `(1,391,323)` because we specified the `time` dimension should have chunks of length `1`.\n",
    "  3. The Array has `4` tasks - this is the number of tasks that will be executed in order to load the data. There are `2` chunk tasks, one for each time slice.\n",
    "  4. The Array type is `int16` and is split up into chunks which are `numpy.ndarrays`.\n",
    "  \n",
    "The chunking has split the array loading into two Chunks. __Dask can execute these in parallel.__\n",
    "\n",
    "We can look at the delayed tasks and how they will be executed by visualising the task graph for one of the variables. We'll use the red band measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7751aa4-ef91-437d-afcf-9efebfd0fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f79fb7-b570-4db0-b000-25d73b296413",
   "metadata": {},
   "source": [
    "Details on the task graph can be found in the dask user guide but what's clear is you have two independent paths of execution which produce one time slice each (0,0,0) and (1,0,0) these are the two chunks that that full array has been split into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcb46d3-a854-460e-bfd3-1d2681dab13c",
   "metadata": {},
   "source": [
    "To retrieve the actual data we need to `compute()` the result, this will cause all the delayed tasks to be executed for the variable we are computing. Let's `compute()` the red variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ecc2f-7a93-4006-9c42-bc1222e39d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_red = dataset.nbart_red.compute()\n",
    "actual_red"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197675fa-14b0-4c63-a1b1-fb83ef12df72",
   "metadata": {},
   "source": [
    "As you can see we now have actual data. You can do the same thing for all arrays in the dataset in one go by computing the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547f92d-0546-4c03-ae43-56b0e54b2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_dataset = dataset.compute()\n",
    "actual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212dd20-baa1-4e62-a7b0-fe4cd410f60b",
   "metadata": {},
   "source": [
    "## The impact of dask on ODC\n",
    "\n",
    "From the above we can see that specifying `dask_chunks` in `datacube.load()` splits up the `load()` operation into a set of `chunk` shaped arrays and `delayed` _tasks_. Dask can now perform those tasks in _parallel_. Dask will only _compute_ the results for those parts of the data we are using but we can force the computation of all the `delayed` _tasks_ using `compute()`.\n",
    "\n",
    "There is a _lot_ more opportunity than described in this simple example but let's just focus on the impact of dask on ODC for this simple case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51187d37-24bb-4887-9109-77d53165b8ba",
   "metadata": {},
   "source": [
    "The time period and ROI are far to small to be interesting so let's change our time range to a full year of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854dacbe-a68a-4b93-820f-2330da2b2510",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_time = (\"2021-01-01\", \"2021-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0394ad17-80e0-40f3-8c27-8d3dc2b5c35f",
   "metadata": {},
   "source": [
    "First load the data without dask (no `dask_chunks` specified), this will take several minutes so be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174dfdba-8708-487c-a6a7-149143191e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277a369-9d56-4396-bcd9-b968efc0779c",
   "metadata": {},
   "source": [
    "46 time observations and in the order of 4-5 minutes to load.\n",
    "\n",
    "Let's enable dask and repeat the load. We're chunking by time (length one) so dask will be able to load each time slice in parallel. The data variables are also independent so will be done in parallel as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57bd8a-84a2-4ec6-a7d7-974c84c2524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"time\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2aa626-f2ae-49af-84bb-bcc189ed78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, ###### THIS IS THE ONLY LINE CHANGED. #####\n",
    "            group_by=group_by,\n",
    "        )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844e75c-2f25-41cb-bcc5-b3f1dc0c01f6",
   "metadata": {},
   "source": [
    "Woah!! that was fast - but we didn't actually compute anything so no load has occurred and all tasks are pending.\n",
    "Open up the Data Variables, click the stacked cylinders and take a look at the delayed task counts. These exist for every variable.\n",
    "\n",
    "Let's visualise the _task graph_ for the `nbart_red` band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5584957c-10b1-470c-b5d3-135ed773c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.nbart_red.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444fb54-be18-4732-b58e-d51101f3b2bc",
   "metadata": {},
   "source": [
    "Well that's not as useful, is it!\n",
    "\n",
    "You should just be able to make out that each of the _chunks_ are able to independently `load()`. `time` _chunk_ is length 1 so these are individual times. This holds true for all the bands so dask can spread these out across multiple threads.\n",
    "\n",
    "> __Tip__: Visualising task graphs is less effective as your task graph complexity increases. You may need to use simpler examples to see what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800e8fd-21dc-4c3b-ba58-a09ed309cf3b",
   "metadata": {},
   "source": [
    "Let's get the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3e0a7b-1147-4cdc-901a-8b247ff01864",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "actual_dataset = dataset.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ce13e-3ba3-45ba-8122-97031a3871b3",
   "metadata": {},
   "source": [
    "How fast this step is will depend on how many cores are in your Jupyter notebook's local cluster. For an 8-core cluster the `datacube.load()` is taking roughly a 1/4 of the time compared to without `dask`. This is great!\n",
    "\n",
    "Why not 1/8 of time?\n",
    "\n",
    "Dask has overheads, and `datacube.load()` itself is IO bound. There are all sorts of things that result in limits and part of the art of parallel computing is tuning your algorithm to reduce the impact of these and achieve greater performnance. As we scale up this example we'll explore some of these.\n",
    "\n",
    "> __Tip__: Do not expect 8x as many cores to produce 8x the speed up. Algorithms can be tuned to perform better (or worse) as scale increases. This is part of the art of parallel programming. Dask does it's best, and you can often do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9c436-50cd-4768-9ad9-0a769c805777",
   "metadata": {},
   "source": [
    "# Exploiting delayed tasks\n",
    "\n",
    "Now let's repeat the full example, with NDVI calculation and masking, but this time with `dask` and `compute` to load the data in.\n",
    "\n",
    "First the `dc.load()`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cd8005-f5d7-42c5-b44a-12dd09cd4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = {\"time\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437b3e5-6583-4c41-a513-802089632398",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks,\n",
    "            group_by=group_by,\n",
    "        )\n",
    "actual_dataset = dataset.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93b643-9c66-44b5-af7e-0108c25ee426",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4282e81-fd1b-4d1b-9f8a-0fe9cf39c1d1",
   "metadata": {},
   "source": [
    "Now use the `actual_result` to compute the NDVI for all observation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122a845-454d-41a2-be7a-78e5b6fce0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(actual_dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = actual_dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d77837-1fa0-4d69-bfd8-78acfb10fdf5",
   "metadata": {},
   "source": [
    "Most of the time is in IO, the actual calculation is < 1 second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f5933-d496-4886-84dd-22e7c3d93dd5",
   "metadata": {},
   "source": [
    "Now let's repeat that entire load and NDVI calculation in a single cell and time it - this is just to get the total time for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28df278f-c09c-43ed-b6dd-e6c333a45a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"oa_fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, \n",
    "            group_by=group_by,\n",
    "        )\n",
    "actual_dataset = dataset.compute() ### Compute the dataset ###\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(actual_dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = actual_dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c86f18-2573-447e-95e1-c03c36064744",
   "metadata": {},
   "source": [
    "40-50 seconds (for an 8-core cluster) or so. We can do better..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37f9bb-b82f-49f8-8d29-b45b42398821",
   "metadata": {},
   "source": [
    "## Data and computational locality\n",
    "\n",
    "When `compute()` is called `dask` not only executes all the tasks but it consolidates all the distributed chunks back into a normal array on the client machine - in this case the notebook's kernel. In the previous cell we have two variables that both refer to the data we are loading:\n",
    "1. _dataset_ refers to the `delayed` version of the data. The `delayed` _tasks_ and the _chunks_ that make it up will be __on the cluster__\n",
    "2. _actual_result_ refers to the actual array in the notebook kernel memory after execution of the _tasks_. The _actual_result_ is a complete array in memory in the notebook kernel (__on the _client___).\n",
    "\n",
    "So in the previous cell everything _after_ the `actual_dataset = dataset.compute()` line is computed in the Jupyter kernel and doesn't use the dask cluster at all for computation.\n",
    "\n",
    "If we shift the location of this `compute()` call we can perform more _tasks_ in parallel on the dask cluster. \n",
    "\n",
    "> __Tip__: Locality is an important concept and applies to both data and computation\n",
    "\n",
    "Now let's repeat the load and NDVI calculation but this time rather than `compute()` on the full `dataset` we'll run `cloud_free = dataset.where(cloud__free_mask).compute()` so the masking operation can be performed in parallel. Let's see what the impact is...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe23d9a-6db9-476e-a1a6-39da48eadb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"oa_fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, \n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask).compute()    ### COMPUTE MOVED HERE ###\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "actual_ndvi = ndvi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b429a6-3cce-41f5-9e59-8455b2740ea8",
   "metadata": {},
   "source": [
    "Not that different... Not too surprising since the masking operation is pretty quick (it's all numpy) and the IO is the bulk of the processing.\n",
    "\n",
    "Dask can see the entire task graph for both load and mask computation. As a result _some_ of the computation can be performed concurrently with file IO, and CPUs are busier as a result, so it will be slightly faster in practice but with IO dominating we won't see much overall improvement.\n",
    "\n",
    "Perhaps doing more of the calculation on the cluster will help. Let's also move `ndvi.compute()` so the entire calculation is done on the cluster and only the final result returned to the client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f58e6-5b88-4fa1-9ed9-e0fa302b0c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"oa_fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, \n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "actual_ndvi = ndvi.compute()    ### COMPUTE MOVED HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd24f8-d3e5-4abd-bc59-067269af1284",
   "metadata": {},
   "source": [
    "Now we are seeing a huge difference!\n",
    "\n",
    "You may be thinking \"Hold on a sec, the NDVI calculation is pretty quick in this example with such a small dataset, why such a big difference?\" - and you'd be right. There is more going on.\n",
    "\n",
    "Remember that `dataset` is a _task graph_ with `delayed` tasks waiting to be executed __when the result is required__. In the example `dataset`, 22 data variables are available but _only 3 are used_ to produce the `ndvi` (`oa_fmask`, `nbart_red` and `nbart_nir`). As a result _`dask` doesn't load the other 19 variables_ and because computation time in this case is mostly IO related the execution time is a LOT faster.\n",
    "\n",
    "Of course we can save `dask` the trouble of figuring this out on our behalf and only `load()` the `measurements` we need in the first place. Let's check that now, we should see a similar performance figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8ff2b-d2a7-4c2a-ada4-da56ee1b71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "measurements = [ \"oa_fmask\", \"nbart_red\", \"nbart_nir\"]\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"oa_fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, \n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n",
    "actual_ndvi = ndvi.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6669d68-5d23-4769-83e1-4f1a06ada5fe",
   "metadata": {},
   "source": [
    "Pretty similar as expected.\n",
    "Now it can pay to give `dask` a hand and not have the _task graph_ cluttered with tasks you are not going to use. Still it's nice to see that `dask` can save you some time by only computing what is required when you need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217f4d21-1e94-4891-95fe-0200f940224f",
   "metadata": {},
   "source": [
    "# A quick check on the task graph\n",
    "\n",
    "For completeness we will take a look at the _task graph_ for the full calculation, all the way to the NDVI result. Given the complexity of the full graph we'll simplify it to 2 time observations like we did when the task graph was introduced previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722aa50-a7c9-46c7-9bd1-ee8ebea54f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_time = (\"2021-01-01\", \"2021-01-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b4922-71f3-418b-81af-f3e0c0d15fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataset = None # clear results from any previous runs\n",
    "measurements = [ \"oa_fmask\", \"nbart_red\", \"nbart_nir\"]\n",
    "dataset = dc.load(\n",
    "            product=products,\n",
    "            x=study_area_lon,\n",
    "            y=study_area_lat,\n",
    "            time=set_time,\n",
    "            measurements=measurements,\n",
    "            resampling={\"oa_fmask\": \"nearest\", \"*\": \"average\"},\n",
    "            output_crs=set_crs,\n",
    "            resolution=set_resolution,\n",
    "            dask_chunks = chunks, \n",
    "            group_by=group_by,\n",
    "        )\n",
    "\n",
    "# Identify pixels that are either \"valid\", \"water\" or \"snow\"\n",
    "cloud_free_mask = (\n",
    "    masking.make_mask(dataset.oa_fmask, fmask=\"valid\")\n",
    ")\n",
    "# Apply the mask\n",
    "cloud_free = dataset.where(cloud_free_mask)\n",
    "\n",
    "# Calculate the components that make up the NDVI calculation\n",
    "band_diff = cloud_free.nbart_nir - cloud_free.nbart_red\n",
    "band_sum = cloud_free.nbart_nir + cloud_free.nbart_red\n",
    "# Calculate NDVI and store it as a measurement in the original dataset ta da\n",
    "ndvi = None\n",
    "ndvi = band_diff / band_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabc0f2-5899-4900-b73c-6a46a3286ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi.data.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27734b-602e-4a3d-9758-d6eca5e7cb85",
   "metadata": {},
   "source": [
    "The computation flows from bottom to top in the _task graph_. You can see there are two main paths, one for each time (since the time chunk is length 1). You can also see the three data sources are loaded independently. After that it gets a little more difficult to follow but you can see `oa_fmask` being used to produce the mask (and, eq_). Then combined via the `where` function with other two datasets. Then finally the NDVI calculation - a sub, add and divide (truediv).\n",
    "\n",
    "Dask has lots of internal optimizations that it uses to help identify the dependencies and parallel components of a task graph. Sometimes it will reorder or prune operations where possible to further optimise (for example, not loading _data variables_ that aren't used in the NDVI calculation).\n",
    "\n",
    "> __Tip__: The _task graph_ can be complex but it is a useful tool in understanding your algorithm and how it scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b3d8d9-1a40-4d17-a5a1-601ba5dda563",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Be a good dask user - Clean up the cluster resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeb0ee-6888-4362-8615-19cf217e80c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12927511-160b-44af-aa81-a6ed137e5fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
